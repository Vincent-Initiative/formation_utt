{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eW-EbO4V9xT"
   },
   "source": [
    "# Light EDA on movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbLjiyVfV9xV"
   },
   "source": [
    "## 1. MovieLens datasets: load & access\n",
    "\n",
    "Spark lets you explore data of any structure from a lot of different data sources and data formats.\n",
    "\n",
    "To load the data, upload them in the data section on the left pane.\n",
    "**Please only upload the smallest dataset.**\n",
    "\n",
    "You should get the paths to access the data from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "error",
     "timestamp": 1699207464695,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "E5Gu4iORV9xV",
    "outputId": "e542820c-7be3-4f06-dd9c-816ef7514091"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "movies_path = \"../data/movies.csv\"\n",
    "ratings_path = \"../data/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noXkW2n7V9xW"
   },
   "source": [
    "Print first record in the movies.csv and ratings.csv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464695,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "sjlT_XOCV9xX"
   },
   "outputs": [],
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hfl0j1RnV9xX"
   },
   "source": [
    "The structure is CSV (comma separated values) and is well-documented (see links below) but we'll be assuming that we don't even know the structure.\n",
    "\n",
    "- Small dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "- Big dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-README.html\n",
    "\n",
    "We will use two files from this MovieLens dataset: *ratings.csv* and *movies.csv*. All ratings are contained in the file *ratings.csv* and are in the following format:\n",
    "```\n",
    "userId,movieId,rating,timestamp\n",
    "```\n",
    "Movies information are in the file *movies.csv* and are in the following format:\n",
    "```\n",
    "movieId,title,genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IzvclXhV9xX"
   },
   "source": [
    "Now that you are able to access the data, let's explore Spark functionalities.\n",
    "\n",
    "As you probably know any Spark session needs a SparkSession to submit jobs to an executors cluster. On this managed environment you were provided a free trial Spark cluster and a SparkSession is already available as **spark**.\n",
    "\n",
    "Refer to the Spark Python API documentation to learn what method you can call on SparkSession object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464695,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "qH7q1cuKV9xX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Spark version used in this notebook\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464695,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "oDxiHT5RV9xY"
   },
   "outputs": [],
   "source": [
    "# 1. Load the 2 datatets\n",
    "# Your code below\n",
    "\n",
    "# 2. The 2 datasets will be reused several times, what could we do to avoid re-reading the files ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ5m6CKHV9xZ"
   },
   "source": [
    "It was fast but remember that nothing happened yet. Spark just began to build an execution plan but is waiting you to provide an action before executing anything. The DataFrames are however ready to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te8ND0xMV9xZ"
   },
   "source": [
    "## 2. Spark basics\n",
    "Let's discover Spark through simple commands first. Let say we know nothing about the dataset we just loaded. Those data could be unstructured, semi-structured or structured and contain any data format. Spark does not really care, the **read.text()** method let you load those files in DataFrames and each line of those files is now an element of the DataFrames.\n",
    "\n",
    "From this chapter, you will find some exercices. The places where you have to put code are marked with **#TODO: explanation**.\n",
    "\n",
    "First thing you want to know is what is in your dataset, how many elements do you have, what is the structure, the attributes types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "SF5ulZbgV9xZ"
   },
   "outputs": [],
   "source": [
    "# Display schema\n",
    "# Your code below\n",
    "\n",
    "# Show first items\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg2n144rV9xZ"
   },
   "source": [
    "The *movies* DataFrame seems to be in CSV format and it is good to know there is an header.\n",
    "\n",
    "But to understand the data types, you probably want to get more lines. Use the Spark Python API documentation to find out how to retrieve 10 lines from both datasets *ratings* and *movies*.\n",
    "\n",
    "Notice that you probably don't want to retrieve **all lines**. In distributed computation, the dataset could be huge and it's probably a bad thing to retrieve all the data from executors on hundreds of machine to the driver on one single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "BZ0zZrc5V9xZ"
   },
   "outputs": [],
   "source": [
    "# Exercice 1: get 10 elements from every dataset\n",
    "rats = # TODO: get 10 elements\n",
    "print(\"--------\\nRatings:\\n--------\")\n",
    "print(rats)\n",
    "\n",
    "movs = # TODO: get 10 elements\n",
    "print(\"\\n--------\\nMovies:\\n--------\")\n",
    "print(movs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xk9tpeRV9xa"
   },
   "source": [
    "We notice that ratings elements are strings with comma separated values. The values are integers or floats.\n",
    "\n",
    "About movies, elements are strings with comma separated values. The values are strings, possibly with pipe separated values (for categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "NJnrJit0V9xa"
   },
   "outputs": [],
   "source": [
    "# Exercice 2: print the number of elements in every dataset\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATAT8FwdV9xa"
   },
   "source": [
    "The biggest dataset has 22M+ elements. If we experience computing delays, we may prefer work on the smaller dataset.\n",
    "\n",
    "While you are working in Spark with data from an input file, you usually start with this kind of Dataframes of *rows* from your input file. But this input file probably has a structure or some specific elements that you want to extract from it in order to give your Spark RDD a structure. For example, this CSV file has four attributes: userId, movieId, rating and timestamp. Spark's DataFrames does not understand the data structure but you can give one to your data by splitting the lines on the comma separator.\n",
    "\n",
    "Prepare the DataFrame by extracting the different fields and removing the header row and the timestamp field. You can also cast the fields in integer and float. Start with the small dataset, check the final RDD with **first()** or **take()**. The **map()** method is the DataFrame's method that you are looking for if you wish to apply a function to any element of a DataFrame and get another DataFrame in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "1KioP4jbV9xa"
   },
   "outputs": [],
   "source": [
    "# Exercice 3: prepare the DataFrames\n",
    "# Movies\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "IkL_04ebV9xb"
   },
   "outputs": [],
   "source": [
    "# Ratings\n",
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464696,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "U8ny9DXcV9xb"
   },
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZdNmKAAV9xb"
   },
   "source": [
    "We had a *ratings* DataFrame of strings representing lines in our input file.\n",
    "\n",
    "We now have a *ratings_df* DataFrame of (integer, integer, float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rJV-9HjV9xb"
   },
   "source": [
    "With those DataFrames, it will be easier to answer the two following exercices. In fact, it would be even easier if you were familiar with SQL (Standard Query Language) by the abstraction of DataFrames. Let's do it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "W3TCLiQYV9xb"
   },
   "outputs": [],
   "source": [
    "# Exercice 4: how many different users is there in the dataset and how many movies have been rated?\n",
    "num_users = ## TODO ##\n",
    "\n",
    "num_movies = ## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "54PU63E6V9xb"
   },
   "outputs": [],
   "source": [
    "# Exercice 5: what are the maximum rating and the minimum rating that appear in the dataset?\n",
    "# Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "BJafFee6V9xc"
   },
   "outputs": [],
   "source": [
    "# Exercice 6: give the full distribution of the ratings, ie. number of occurences of each rating, you can help yourself with the WordCount example\n",
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayl1AwW9V9xc"
   },
   "source": [
    "In the previous code, it is important to understand where the code executes. You should take advantage of your Spark's cluster power whenever possible and only manipulates small datasets on the driver single machine.\n",
    "\n",
    "Notice the distribution of the ratings is not uniform. We can represent it with a Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "lf-3ZebbV9xc"
   },
   "outputs": [],
   "source": [
    "ratings_distribution.display(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "8-9Gwv-iV9xc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pandas\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This line needs to be changed with your variable name\n",
    "distribution_pandas = ratings_distribution.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,12));\n",
    "distribution_pandas['count'].plot(kind=\"bar\")\n",
    "ax.set_xticklabels(distribution_pandas['rating']);\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207464697,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "wdCZAeMPV9xc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/Vincent-Initiative/formation_utt_solutions/blob/main/solutions/1_movielens_exercises_solutions.ipynb",
     "timestamp": 1699204736249
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "name": "exercices",
  "notebookId": 1173226566346544
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
