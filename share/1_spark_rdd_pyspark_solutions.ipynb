{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U1Wx2D6fPVl"
   },
   "source": [
    "#  We are getting started with Apache Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NZK8C-MfPVm"
   },
   "source": [
    "# Apache Spark RDD\n",
    "## Step 1: Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1699207502677,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "JeNttWbEfPVn",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a sparkSession\n",
    "# Your code below\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4GCaxD5fPVo"
   },
   "source": [
    "## Step 2: Create a rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699207503024,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "lL47mNyafPVo"
   },
   "outputs": [],
   "source": [
    "data = range(1,1000)\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1699207503478,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "DwGsJ8H6fPVo",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "01403692-9dfc-4ed1-d82f-38515a3983bf"
   },
   "outputs": [],
   "source": [
    "# Display the data, try several methods, what do they do, what are the differences ?\n",
    "# Your code below\n",
    "rdd.collect()\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1699207503738,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "yV_hAbyifPVp",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6906094d-7fcc-4922-9a1a-4ecc285e3ce5"
   },
   "outputs": [],
   "source": [
    "# Notice the return type of the methods you used\n",
    "# Your code below\n",
    "type(rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZN44bGpfPVq"
   },
   "source": [
    "We have 2 parallel operations in RDD which are Transformation and Action. Transformation and Action were already discussed briefly earlier. So let’s see how transformation works. Remember that RDDs are immutable – so we can’t change our RDD, but we can apply transformation on it. Let’s see an example of map transformation to demonstrate how transformation works.\n",
    "\n",
    "## Step 3: Map transformation.\n",
    "\n",
    "Map transformation returns a Mapped RDD by applying function to each element of the base RDD. Let’s repeat the first step of creating a RDD from existing source, For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699207503738,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "dya7ZM5rfPVq"
   },
   "outputs": [],
   "source": [
    "data = ['Hello' , 'I' , 'am', 'a', 'big', 'data', 'engineer!']\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EuprAsIfPVr"
   },
   "source": [
    "Now a RDD (name is ‘rdd’) is created from the existing source, which is a list of strings in a driver program. We will now apply a lambda function to each element of rdd and return the mapped (transformed) RDD (word,1) pair in the mappedRdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1699207503738,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "uVyC1RUffPVr",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mappedRdd = rdd.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ite4c1EBfPVr"
   },
   "source": [
    "Lazy evaluation, the previous statement is only evaluated when we perform the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1699207504075,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "a5ZrjZMYfPVr",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "c03c118d-2dbe-4352-c731-d8b2114f3e4f"
   },
   "outputs": [],
   "source": [
    "mappedRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RglfjfkfPVs"
   },
   "source": [
    "Nothing happened after applying the lambda function on rdd1 (we won’t see any computation happening in a cluster). This is called the lazy operation. All transformation operations in Spark are lazy, which means that we will not see any computations on RDD, until we need them for further action.\n",
    "\n",
    "Spark remembers which transformation is applied to which RDD with the help of DAG (Directed a Cyclic Graph). The lazy evaluation helps Spark to optimize the solution because Spark will get time to see the DAG before actually executing the operations on RDD. This enables Spark to run operations more efficiently.\n",
    "\n",
    "In the code above, collect() and take() are the examples of an action.\n",
    "\n",
    "There are many number of transformation defined in Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpwmleE5fPVs"
   },
   "source": [
    "\n",
    "* We can create a RDD in two different ways, from existing source and external source.\n",
    "* We can apply two types of operations on RDD, namely “transformation” and “action”. All transformations on RDD are lazy in nature, which means that computations on RDD are not done until we apply an action.\n",
    "* RDDs are immutable in nature i.e. we cannot change the RDD, we need to transform it by applying transformation(s). There are various transformations and actions, which can be applied on RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYphhK__fPVs"
   },
   "source": [
    "What is Transformation and Action?\n",
    "\n",
    "Spark has certain operations which can be performed on RDD. An operation is a method, which can be applied on a RDD to accomplish certain tasks. RDD supports two types of operations, which are Action and Transformation. An operation can be something as simple as sorting, filtering and summarizing data.\n",
    "\n",
    "Let’s take few examples to understand the concept of transformation and action better. Let’s assume, we want to develop a machine learning model on a data set. Before applying a machine learning model, we will need to perform certain tasks:\n",
    "\n",
    "* Understand the data ( List out the number of columns in data and their type)\n",
    "* Preprocess the data (Remove null value observations on data).\n",
    "* Filter the data (Let’s say, we want to filter the observations corresponding to males data)\n",
    "* Fill the null values in data ( Filling the null values in data by constant, mean, median, etc)\n",
    "* Calculate the features in data\n",
    "\n",
    "All the above mentioned tasks are examples of an operation. In Spark, operations are divided into 2 parts – one is transformation and second is action. Find below a brief descriptions of these operations.\n",
    "\n",
    "Transformation: Transformation refers to the operation applied on a RDD to create new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "Actions: Actions refer to an operation which also applies on RDD, that instructs Spark to perform computation and send the result back to driver. This is an example of action.\n",
    "\n",
    "The Transformations and Actions in Apache Spark are divided into 4 major categories:\n",
    "\n",
    "* General\n",
    "* Mathematical and Statistical\n",
    "* Set Theory and Relational\n",
    "* Data-structure and IO\n",
    "\n",
    "\n",
    "Applying Transformation and Action\n",
    "\n",
    "To understand the operations, we are going to use the readme textfiel.  I have already copied and pasted all text. Before applying operations on the readme, we need to first load this file with the help of SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1699207504331,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "XKWXnTEtfPVt",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"../data/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 378,
     "status": "error",
     "timestamp": 1699207504708,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "fpGqNFW3fPVt",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "3d517356-41c6-4823-8106-9d613c474b0c"
   },
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKuei-9bfPVt"
   },
   "source": [
    "## Transformation: map and flatMap\n",
    "\n",
    "Q1: Convert all words in a rdd to lowercase and split the lines of a document using space.\n",
    "\n",
    "To lower the case of each word of a document, we can use the map transformation. A map transformation is useful when we need to transform a RDD by applying a function to each element. So how can we use map transformation on ‘rdd’ in our case?\n",
    "\n",
    "Solution: Let’s see through the example, Apply a function called “Func” on each words of a document. “Func” will do two things:\n",
    "\n",
    "1. It will take a corpus, lower the each words in this corpus.\n",
    "2. After that it splits the words in each line by space.\n",
    "\n",
    "To do this first we need to write “Func” and then apply this function using map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207504708,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "dfW5Mx38fPVt"
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC047dLgfPVu"
   },
   "source": [
    "After applying the function (Func) on “rdd”, we have transformed this “rdd” into “rdd1”, we can see the first 5 elements of “rdd1” by applying take operation (which is an action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207504709,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "WeRl_h2XfPVu",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8epfl1qMfPVu"
   },
   "source": [
    "We can also see that our output is not flat (it’s a nested list). So for getting the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap” will help here:\n",
    "\n",
    "The “flatMap” transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the “flatMap” and map transformations. Let’s apply a “flatMap” transformation on “rdd” , then take the result of this transformation in “rdd2” and print the result after applying this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207504709,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "d4gLDNDSfPVu",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(Func)\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-PzI48-fPVu"
   },
   "source": [
    "# Transformation: filter\n",
    "\n",
    "Q2: Next, I want to remove the words, which are not necessary to analyze this text. We call these words as “stop words”; Stop words do not add much value in a text. For example, “is”, “am”, “are” and “the” are few examples of stop words.\n",
    "\n",
    "Solution: To remove the stop words, we can use a “filter” transformation which will return a new RDD containing only the elements that satisfy given condition(s). Lets apply “filter” transformation on “rdd2” and get words which are not stop words and get the result in “rdd3”. To do that:\n",
    "\n",
    "1. We need to define the list of stop words in a variable called “stopwords” ( Here, I am selecting only a few words in stop words list instead of all the words).\n",
    "2. Apply “filter” on “rdd2” (Check if individual words of “rdd2” are in the “stopwords” list or not ).\n",
    "\n",
    "We can check first 10 elements of “rdd3” by applying take action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1699207504709,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "LG4MsOwjfPVv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'and', 'to']\n",
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl2XaPGjfPVv"
   },
   "source": [
    "## Transformation: groupBy\n",
    "\n",
    "Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.\n",
    "\n",
    "Solution: The “groupBy”  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.\n",
    "\n",
    "1. We have to pass a function (in this case, I am using a lambda function) inside the “groupBy” which will take the first 3 characters of each word in “rdd3”.\n",
    "2. The key is the first 3 characters and value is all the words which start with these 3 characters.\n",
    "\n",
    "After applying “groupBy” function, we store the transformed result in “rdd4” (RDDs are immutable – remember!). To view “rdd4”, we can print first (key, value) elements in “rdd4”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207504709,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "1vnq994qfPVv",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkQB4Cd7fPVv"
   },
   "source": [
    "# Transformation: groupByKey / reduceByKey\n",
    "\n",
    "Q4: What if we want to calculate how many times each word is coming in corpus ?\n",
    "\n",
    "Solution: We can apply the “groupByKey” / “reduceByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.\n",
    "\n",
    "To use “groupbyKey” / “reduceByKey” transformation to find the frequencies of each words, you can follow the steps below:\n",
    "\n",
    "1. A (key,val) pair RDD is required; In this (key,val) pair RDD, key is the word and val is 1 for each word in RDD (1 represents the number for the each word in “rdd3”).\n",
    "2. To apply “groupbyKey” / “reduceByKey” on “rdd3”, we need to first convert “rdd3” to (key,val) pair RDD.\n",
    "\n",
    "\n",
    "\n",
    "Let’s see, how to convert “rdd3” to new mapped (key,val) RDD. And then we can apply “groupbyKey” / “reduceByKey” transformation on this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1699207504709,
     "user": {
      "displayName": "Vincent G.",
      "userId": "15679613728828836905"
     },
     "user_tz": -60
    },
    "id": "md_Bh5_3fPVv"
   },
   "outputs": [],
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/reduceByKey-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in case of “groupByKey” transformation, it will not combine the values in each key in all partition it directly shuffle the data then merge the values for each key. Here in “groupByKey” transformation lot of shuffling in the data is required to get the answer, so it is better to use “reduceByKey” in case of large shuffling of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/groupbykey.png\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/Vincent-Initiative/formation_utt_solutions/blob/main/solutions/0_spark_rdd_pyspark_solutions.ipynb",
     "timestamp": 1699204706994
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
