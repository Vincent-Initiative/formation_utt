{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens notebook\n",
    "\n",
    "Thanks to Victor Hatiguais for initiating this notebook.\n",
    "\n",
    "In this notebook, we will use Spark MLlib to build a recommender system from MovieLens datasets.\n",
    "\n",
    "MovieLens is a project by GroupLens, a research laboratory at University of Minnesota, to provide a movies recommender application and use the collected data to improve algorithms. On https://movielens.org/, anyone can try the app for free and get movies recommandations. To help many people develop the best recommandation algorithms, MovieLens also released several datasets on http://grouplens.org/datasets/movielens/. We will use those datasets in this notebook.\n",
    "\n",
    "We will work with the two latest datasets available on MovieLens. The smallest one will help us build our application as fast as possible but you can use the biggest one whenever you want if you'd like to experience Spark power with a bigger dataset. Please keep in mind that we'll be using a free low capacity Spark cluster. Spark's scalability lets you run the same exact code on a much bigger cluster if you wish.\n",
    "\n",
    "The files to be uploaded from the MovieLens latest small dataset are:\n",
    "- movies.csv\n",
    "- ratings.csv\n",
    "Additional files may be uploaded depending on exercises, such as :\n",
    "- moviesBig.csv\n",
    "- ratingsBig.csv\n",
    "\n",
    "The small dataset is around 100k ratings. The biggest one is around 22M ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Jupyter & Markdown\n",
    "\n",
    "First, here are some links with useful information to help you answer the following exercices.\n",
    "\n",
    "This is a Databricks notebook environment where you can interactively develop Spark programs: https://docs.databricks.com/user-guide/notebooks/index.html\n",
    "\n",
    "Markdown is a simple markup language to structure text: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "\n",
    "Additional useful resources include:\n",
    "- Spark Python API documentation: http://spark.apache.org/docs/latest/api/python/\n",
    "    - SparkContext: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "    - DataFrame: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.DataFrame\n",
    "    - sqlContext: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext\n",
    "    - DataFrame: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. MovieLens datasets: load & access\n",
    "\n",
    "Spark lets you explore data of any structure from a lot of different data sources and data formats.\n",
    "\n",
    "To load the data, upload them in the data section on the left pane.\n",
    "**Please only upload the smallest dataset.**\n",
    "\n",
    "You should get the paths to access the data from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first record in the movies.csv and ratings.csv datasets\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using data stored as text files in the Databricks File System. The structure is CSV (comma separated values) and is well-documented (see links below) but we'll be assuming that we don't even know the structure.\n",
    "\n",
    "- Small dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html\n",
    "- Big dataset documentation: http://files.grouplens.org/datasets/movielens/ml-latest-README.html\n",
    "\n",
    "We will use two files from this MovieLens dataset: *ratings.csv* and *movies.csv*. All ratings are contained in the file *ratings.csv* and are in the following format:\n",
    "```\n",
    "userId,movieId,rating,timestamp\n",
    "```\n",
    "Movies information are in the file *movies.csv* and are in the following format:\n",
    "```\n",
    "movieId,title,genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are able to access the data, let's explore Spark functionalities.\n",
    "\n",
    "As you probably know any Spark session needs a SparkSession to submit jobs to an executors cluster. On this managed environment you were provided a free trial Spark cluster and a SparkSession is already available as **spark**.\n",
    "\n",
    "Refer to the Spark Python API documentation to learn what method you can call on SparkSession object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display Spark version used in this notebook\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the 2 datatets\n",
    "## TODO ##\n",
    "\n",
    "# 2. The 2 datasets will be reused several times, what could we do to avoid re-reading the files ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was fast but remember that nothing happened yet. Spark just began to build an execution plan but is waiting you to provide an action before executing anything. The DataFrames are however ready to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark basics\n",
    "Let's discover Spark through simple commands first. Let say we know nothing about the dataset we just loaded. Those data could be unstructured, semi-structured or structured and contain any data format. Spark does not really care, the **read.text()** method let you load those files in DataFrames and each line of those files is now an element of the DataFrames.\n",
    "\n",
    "From this chapter, you will find some exercices.\n",
    "\n",
    "First thing you want to know is what is in your dataset, how many elements do you have, what is the structure, the attributes types, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "## TODO ##\n",
    "\n",
    "# Show first items\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *movies* DataFrame seems to be in CSV format and it is good to know there is an header.\n",
    "\n",
    "But to understand the data types, you probably want to get more lines. Use the Spark Python API documentation to find out how to retrieve 10 lines from both datasets *ratings* and *movies*.\n",
    "\n",
    "Notice that you probably don't want to retrieve **all lines**. In distributed computation, the dataset could be huge and it's probably a bad thing to retrieve all the data from executors on hundreds of machine to the driver on one single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1: get 10 elements from every dataset\n",
    "ratings_10 = ## TODO ##\n",
    "print(\"--------\\nRatings:\\n--------\")\n",
    "print(ratings_10)\n",
    "\n",
    "movies_10 = ## TODO ##\n",
    "print(\"\\n--------\\nMovies:\\n--------\")\n",
    "print(movies_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that ratings elements are strings with comma separated values. The values are integers or floats.\n",
    "\n",
    "About movies, elements are strings with comma separated values. The values are strings, possibly with pipe separated values (for categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2: print the number of elements in every dataset\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest dataset has 22M+ elements. If we experience computing delays, we may prefer work on the smaller dataset.\n",
    "\n",
    "While you are working in Spark with data from an input file, you usually start with this kind of Dataframes of *rows* from your input file. But this input file probably has a structure or some specific elements that you want to extract from it in order to give your Spark RDD a structure. For example, this CSV file has four attributes: userId, movieId, rating and timestamp. Spark's DataFrames does not understand the data structure but you can give one to your data by splitting the lines on the comma separator.\n",
    "\n",
    "Prepare the DataFrame by extracting the different fields and removing the timestamp field. You can also cast the fields in integer and float. Start with the small dataset, check the final DF with **first()** or **take()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3: prepare the DataFrames\n",
    "# Movies\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a *ratings* DataFrame of strings representing lines in our input file.\n",
    "\n",
    "We now have a *ratings_df* DataFrame of (integer, integer, float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those DataFrames, it will be easier to answer the two following exercices. In fact, it would be even easier if you were familiar with SQL (Standard Query Language) by the abstraction of DataFrames. Let's do it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4: how many different users is there in the dataset and how many movies have been rated?\n",
    "num_users = ## TODO ##\n",
    "\n",
    "num_movies = ## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5: what are the maximum rating and the minimum rating that appear in the dataset?\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6: give the full distribution of the ratings, ie. number of occurences of each rating.\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, it is important to understand where the code executes. You should take advantage of your Spark's cluster power whenever possible and only manipulates small datasets on the driver single machine.\n",
    "\n",
    "Notice the distribution of the ratings is not uniform. We can display it with the display method provided by Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_distribution.display(\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pandas\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This line needs to be changed with your variable name\n",
    "distribution_pandas = ratings_distribution.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,12));\n",
    "distribution_pandas['count'].plot(kind=\"bar\")\n",
    "ax.set_xticklabels(distribution_pandas['rating']);\n",
    "\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "exercices",
  "notebookId": 1173226566346544
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
